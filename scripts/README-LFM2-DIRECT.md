# LFM2-VL-1.6B 直接使用ガイド（Ollama経由ではない）

LFM2-VL-1.6BをOllama経由ではなく、Pythonスクリプト経由で直接使用する方法です。

## 前提条件

1. **Python 3.8以上**がインストールされていること
2. **必要なPythonライブラリ**がインストールされていること：
   ```bash
   pip install transformers pillow torch
   ```

## セットアップ

### 1. Pythonライブラリのインストール

```bash
pip install transformers pillow torch
```

### 2. モデルのダウンロード（初回のみ）

初回実行時、Hugging Faceから自動的にモデルがダウンロードされます。
モデルは `~/.cache/huggingface/hub/` に保存されます。

### 3. 使用方法

アプリケーション内で：
1. トピックモーダルを開く
2. 「ローカルVLMを使用」をチェック
3. VLMモデルで「lfm2-vl-1.6b」を選択
4. 「Ollama経由ではなく直接使用（Pythonスクリプト経由）」をチェック
5. 画像をアップロード

## APIエンドポイント

Next.jsのAPIエンドポイント `/api/lfm2-vl-inference` が自動的にPythonスクリプトを呼び出します。

### リクエスト形式

```json
{
  "image_base64": "base64エンコードされた画像データ",
  "model_path": "LiquidAI/LFM2-VL-1.6B" // オプション
}
```

### レスポンス形式

```json
{
  "success": true,
  "description": "生成された説明文"
}
```

## トラブルシューティング

### Pythonスクリプトが見つからない

エラーメッセージ: `Pythonスクリプトの実行エラー`

解決方法:
- `scripts/lfm2-vl-inference.py` が存在することを確認
- スクリプトに実行権限があることを確認: `chmod +x scripts/lfm2-vl-inference.py`

### 必要なライブラリがインストールされていない

エラーメッセージ: `必要なライブラリがインストールされていません`

解決方法:
```bash
pip install transformers pillow torch
```

### モデルのダウンロードに時間がかかる

初回実行時、モデルファイル（約1.6GB）をダウンロードするため、時間がかかります。
2回目以降は、キャッシュされたモデルが使用されるため、高速に動作します。

### メモリ不足エラー

LFM2-VL-1.6Bは約2GBのメモリを使用します。メモリが不足する場合は：
- より軽量なモデル（Q4_0版）を使用
- システムのメモリを増やす

## パフォーマンス

- **初回実行**: モデルのダウンロードと読み込みに時間がかかります（数分）
- **2回目以降**: キャッシュされたモデルを使用するため、高速に動作します
- **推論速度**: GPU使用時は高速、CPUのみの場合は数秒〜数十秒かかります

## 注意事項

- Pythonスクリプトはサーバーサイド（Next.js APIルート）で実行されます
- ブラウザ環境では直接実行できません（APIエンドポイント経由で呼び出します）
- モデルファイルは初回のみダウンロードされ、以降はキャッシュが使用されます

