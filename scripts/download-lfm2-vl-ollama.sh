#!/bin/bash

# LFM2-VL-1.6Bãƒ¢ãƒ‡ãƒ«ã‚’Hugging Faceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦Ollamaã§ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

MODEL_NAME="LiquidAI/LFM2-VL-1.6B-GGUF"
OLLAMA_MODEL_NAME="lfm2-vl-1.6b"
DOWNLOAD_DIR="./models/lfm2-vl-1.6b"

echo "=========================================="
echo "LFM2-VL-1.6B ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & Ollamaè¨­å®š"
echo "=========================================="
echo ""

# å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ç¢ºèª
echo "ðŸ“‹ å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’ç¢ºèªä¸­..."

if ! command -v ollama &> /dev/null; then
    echo "âŒ ã‚¨ãƒ©ãƒ¼: OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    echo "   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: https://ollama.ai/download"
    exit 1
fi
echo "âœ… Ollama: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿"

if ! command -v huggingface-cli &> /dev/null; then
    echo "âš ï¸  è­¦å‘Š: huggingface-cliãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    echo "   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
    pip install -q huggingface_hub[cli]
    if [ $? -ne 0 ]; then
        echo "âŒ ã‚¨ãƒ©ãƒ¼: huggingface-cliã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ"
        echo "   æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install huggingface_hub[cli]"
        exit 1
    fi
fi
echo "âœ… huggingface-cli: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿"

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p "${DOWNLOAD_DIR}"
DOWNLOAD_DIR_ABS=$(cd "${DOWNLOAD_DIR}" && pwd)
cd "${DOWNLOAD_DIR_ABS}"

echo ""
echo "ðŸ“¥ Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
echo "   ãƒ¢ãƒ‡ãƒ«: ${MODEL_NAME}"
echo "   ä¿å­˜å…ˆ: ${DOWNLOAD_DIR}"
echo ""

# Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# è¤‡æ•°ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã€Q4_0ï¼ˆè»½é‡ç‰ˆï¼‰ã‚’å„ªå…ˆçš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "ðŸ” åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªä¸­..."
huggingface-cli download "${MODEL_NAME}" --local-dir . --local-dir-use-symlinks False

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
echo ""
echo "ðŸ“‚ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:"
ls -lh *.gguf 2>/dev/null || ls -lh *.bin 2>/dev/null || echo "   (ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)"

# GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŽ¢ã™
GGUF_FILE=$(find . -name "*.gguf" -type f | head -n 1)

if [ -z "$GGUF_FILE" ]; then
    echo ""
    echo "âš ï¸  GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    echo "   åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
    echo ""
    echo "   æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ:"
    echo "   huggingface-cli download ${MODEL_NAME} --local-dir ."
    echo ""
    echo "   ã¾ãŸã¯ã€Hugging Faceã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æŽ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:"
    echo "   https://huggingface.co/${MODEL_NAME}"
    exit 1
fi

echo ""
echo "âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: ${GGUF_FILE}"

# çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
ABSOLUTE_GGUF_FILE="${DOWNLOAD_DIR_ABS}/$(basename "${GGUF_FILE}")"
OLLAMA_MODELFILE="${DOWNLOAD_DIR_ABS}/Modelfile"

# Modelfileã®ä½œæˆ
echo ""
echo "ðŸ“ Ollamaç”¨ã®Modelfileã‚’ä½œæˆä¸­..."

cat > "${OLLAMA_MODELFILE}" <<EOF
FROM ${ABSOLUTE_GGUF_FILE}

# LFM2-VL-1.6B ãƒ¢ãƒ‡ãƒ«è¨­å®š
# Liquid AIãŒé–‹ç™ºã—ãŸæ–°ä¸–ä»£ã®ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªžãƒ¢ãƒ‡ãƒ«
# ã‚¨ãƒƒã‚¸AIã‚„ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹ã§ã®å±•é–‹ã«æœ€é©åŒ–

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 4096

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
SYSTEM """
You are LFM2-VL-1.6B, a vision-language model developed by Liquid AI.
You can understand and generate text based on images and text inputs.
"""

TEMPLATE """{{ .System }}

{{ .Prompt }}
"""
EOF

echo "âœ… Modelfileã‚’ä½œæˆã—ã¾ã—ãŸ: ${OLLAMA_MODELFILE}"

# Ollamaã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
echo ""
echo "ðŸš€ Ollamaã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­..."
echo "   ãƒ¢ãƒ‡ãƒ«å: ${OLLAMA_MODEL_NAME}"

# Modelfileã®çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
ABSOLUTE_MODELFILE=$(cd "$(dirname "${OLLAMA_MODELFILE}")" && pwd)/$(basename "${OLLAMA_MODELFILE}")

# æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
if ollama list | grep -q "${OLLAMA_MODEL_NAME}"; then
    echo "âš ï¸  æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å‰Šé™¤ä¸­..."
    ollama rm "${OLLAMA_MODEL_NAME}" || true
fi

# ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
ollama create "${OLLAMA_MODEL_NAME}" -f "${ABSOLUTE_MODELFILE}"

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼"
    echo ""
    echo "ðŸ“ ä½¿ç”¨æ–¹æ³•:"
    echo "   ollama run ${OLLAMA_MODEL_NAME}"
    echo ""
    echo "   ã¾ãŸã¯ã€APIçµŒç”±ã§ä½¿ç”¨:"
    echo "   curl http://localhost:11434/api/generate -d '{\"model\": \"${OLLAMA_MODEL_NAME}\", \"prompt\": \"Hello\"}'"
    echo ""
else
    echo ""
    echo "âŒ ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ"
    echo ""
    echo "   æ‰‹å‹•ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆ:"
    echo "   ollama create ${OLLAMA_MODEL_NAME} -f ${ABSOLUTE_MODELFILE}"
    exit 1
fi

echo "=========================================="
echo "å®Œäº†ï¼"
echo "=========================================="

